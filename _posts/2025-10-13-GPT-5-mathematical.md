---
layout: post
title: Tao Zhexuan solves mathematical problems with GPT-5, only 29 lines of Python code
date: 2025-10-13 10:00:00 +0800
category: Frontier Trends
thumbnail: /style/image/20251013.png
icon: book
---
* content
{:toc}

#Tao Zhexuan solves mathematical problems with GPT-5: only 29 lines of Python code

Pay attention to cutting-edge technology [qubits] (JavaScript: void \ (0 \)); )

AI has once again helped Tao Zhexuan solve a difficult problem!

The message comes from Tao's latest post, in which he bluntly stated:

>Without AI assistance, completing the same task would require several hours of manual coding and debugging.

Even without AI, he would not have decided to adopt the key strategies that have already achieved success.

>In fact, without the help of AI, it would be almost impossible for me to attempt this numerical search * (possibly seeking theoretical asymptotic analysis) *.

![图片](/style/image/2025-10-13/1.png)

Due to the use of GPT-5 * *, OpenAI researcher Sebastien Bubeck * (former Vice President of AI and Distinguished Scientist at Microsoft) * quickly reposted it, sparking heated discussions in the ⼋ community.

![图片](/style/image/2025-10-13/2.png)

In addition to recalling experiences similar to those of Tao Shen himself, netizens all sighed:

>This marks the beginning of a new era where humans and machines are exploring together.

![图片](/style/image/2025-10-13/3.png)

So, what problem did Tao Zhexuan solve with AI this time? How much role has AI played in it?

Let's continue with Kangkang——

##Using only 29 lines of Python code to help validate the results

Tao Zhexuan is going to solve a problem on MathOverflow * (a professional math Q&A community):

>Is the sequence lcm (1,2,..., n) a subset of highly abundant numbers?

![图片](/style/image/2025-10-13/4.png)

Simply put, this problem is actually comparing two special sequences.

One is the least common multiple sequence, such as:

+   n=2，lcm(1,2）=2
    
+   n=3，lcm(1,2,3）=6
    
+   n=4，lcm(1,2,3,4）=12
    
+   n=5，lcm(1,2,3,4,5）=60
    
+   ……
    

The other is the Highly Abundant Numbers (HA) sequence. This type of number has a special property: the sum of all its divisors is greater than any number smaller than it.

For example, the sum of divisors of 1 is 1, the sum of divisors of 2 is 3 * (greater than 1) *, and the sum of divisors of 4 is 7 (greater than both 3 and 1), so they are highly abundant numbers.

Due to the discovery that the least common multiples calculated earlier happen to be highly abundant numbers, the question arises - * * Will all least common multiples always be in the ranks of highly abundant numbers? (i.e. subset)**

![图片](/style/image/2025-10-13/5.png)

For this question, Tao Zhexuan has previously concluded through theoretical analysis that the answer is negative.

However, to fully prove this point, specific numerical parameters need to be found in order to construct a conclusive counterexample.

Although the direction is clear, Tao Zhexuan is aware that due to the tedious calculations and parameter searches required in the process, this can also be considered a "big project" that requires manual labor for several hours.

After being handed over to GPT-5, everything became easier... With just a few rounds of natural language dialogue, AI helped solve the problem.

The specific process is as follows:

At first, Tao Zhexuan attempted to have GPT-5 directly generate Python programs to search for the parameters of these counterexamples.

This way, Tao can run the code and adjust the parameters by himself.

After thinking for about 1 minute and 19 seconds, GPT-5 did indeed provide a very long piece of code.

![图片](/style/image/2025-10-13/6.png)

But in actual operation, it was found that the generated code took too long to run and the initial parameter selection was inappropriate, which ultimately led to search failure.

So he chose to shift his strategy by engaging in a step-by-step dialogue with GPT-5 to use heuristic calculations to find feasible parameter options.

In other words, he chose to break down a big problem into multiple small problems and then, under his own guidance, let GPT-5 break them down one by one.

After several rounds of communication, GPT-5 finally successfully generated the required parameters.

And to ensure the reliability of the results, Tao also used a Python script generated by GPT-5 with 29 lines of code for independent verification.

In Tao's opinion, this script is concise and easy to manually check, and the validated numerical results are completely consistent with the previous heuristic predictions.

```
from math import log

n = 200000;  
#p_j are the first 25 consecutive primes starting at 1.05*sqrt(n)
p = [
479, 487, 491, 499, 503, 509, 521, 523, 541, 547,
557, 563, 569, 571, 577, 587, 593, 599, 601, 607,
613, 617, 619, 631, 641
]

#q_m found by the search (13 primes)
q = [
187963, 187973, 187987, 188011, 188017, 188021, 188029,
188299, 199931, 199933, 199961, 199967, 199999
]

sum_log_p = sum(log(x) for x in p)
sum_log_q = sum(log(x) for x in q)

#Correction terms
sum_corr_p = sum(log(1.0 + 1.0/(x*(x+1))) for x in p)
sum_corr_q = sum(log(1.0 + 1.0/x)          for x in q)

gap_q_minus_p = sum_log_q - sum_log_p
rhs_budget    = sum_corr_p - sum_corr_q

print(f"sum_log_q - sum_log_p = {gap_q_minus_p:.12g}")
print(f"RHS budget            = {rhs_budget:.12g}")
print("Inequality (correct sign) holds?:",
(gap_q_minus_p &gt; 0) and (gap_q_minus_p &lt; rhs_budget))

#If you wanted the opposite sign, this is what you'd be checking:
gap_p_minus_q = -gap_q_minus_p
print(f"\nsum_log_p - sum_log_q = {gap_p_minus_q:.12g}")
print("Inequality (opposite sign) holds?:",
(gap_p_minus_q &gt; 0) and (gap_p_minus_q &lt; rhs_budget))
```

In summary, by using GPT-5, Tao Zhexuan finally completed the negative proof of the above problem.

And he also specifically mentioned that when faced with serious topics like mathematics, AI did not fall into the "old habit" of hallucinations this time.

>I have not encountered any issues with hallucinations or other AI generated gibberish.

![图片](/style/image/2025-10-13/7.png)

##This is not the first time Tao Shen has used AI to solve mathematical problems

In fact, as a top math expert, this is not the first time Tao Zhexuan has used AI to solve math problems.

This year alone, there have been multiple introductions to quantum bits:

+In early September, he used GPT-5 for semi-automatic literature search, which enabled the first conceptual validation of the Erdos problem/OEIS correlation project;
    
+In late May, newcomer blogger Tao Zhexuan demonstrated step by step on YouTube how to prove function limit problems using only GitHub Copilot;
    
+In mid May, during the first oil pipeline show, AI blindly verified the Magma equation E1689 containing E2 in 33 minutes;
    
+In mid March, o3-mini recognized and corrected one of his mistakes at a glance, and with its help, quickly solved a math problem;
    
+   ……
    

In Tao Zhexuan's view, "Artificial intelligence may not win the Fields Medal * (the Nobel Prize in mathematics) * in the short term, but it may serve as an intermediary for mathematicians to prove.

Of course, stepping out of the field of mathematics, Tao Zhexuan's proofing undoubtedly tells us that how to use AI is quite crucial.

![图片](/style/image/2025-10-13/8.png)

## One More Thing

Regarding GPT-5, OpenAI CEO Ultraman's latest statement is causing heated discussions.

>GPT-5 has been misunderstood!

Unlike his previous high-profile stance, he directly told everyone that GPT-5 represents iterative improvement rather than a sudden paradigm shift.

That is to say, people's expectations for GPT-5 are too high * (in response to GPT-5 live streaming failures and complaints about model capabilities not meeting expectations) *.

And when faced with the question of "when to achieve AGI", its attitude has also taken a big turn——

Compared to previously stating that AGI will be achieved before 2030, he has become more cautious this time, emphasizing his greater focus on safety and gradual progress.

Okay, okay, you're a bit of a improviser, aren't you.

Reference link:
\[1\] https://mathstodon.xyz/ @tao/115306424727150237  
\[2\] https://x.com/SebastienBubeck/status/1973977315572154383   
\[3\] https://mathoverflow.net/questions/501066/is-the-least-common-multiple-sequence-textlcm1-2-dots-n-a-subset-of-t/501125